---
layout: post
title: ML  Notes（一） 梯度下降优化算法
---

>将一些纸质的笔记整理出来，也当是复习一下这段时间学习的知识。



# 每次迭代多少个样本

## Batch gradient descent

首先可以想到的是：每次更新可以在整个数据集上计算梯度。  
$$
\theta = \theta - \eta · \nabla_{\theta}J(\theta)
$$
- 迭代一次的速度非常慢。尤其在比较大的数据集上，事实上是实现不了的，因为没有那么大的内存可以让整个数据集都load进去
- 对数据集中相似样本的梯度计算是冗余的

## Stochastic gradient descent（SGD）

另一种极端情况：每次更新只用一个样本来计算梯度。  
$$
\theta = \theta - \eta · \nabla_{\theta}J(\theta;x^{i};y^{i})
$$
![mark](http://p0vxqvs23.bkt.clouddn.com/imgs/171213/JJB74J84Bl.png?imageslim)

- 目标函数在更新的过程中出现严重的抖动（high variance）
  - 好处：可以从一个局部最小跳到另一个局部最小
  - 坏处：迭代好久之后目标函数在最小值附近跳动（overshooting）
- 当我们缓慢的缩小 learning rate, SGD 应该可以达到 Batch gradient descent 的效果

## Mini-batch gradient descent

每次计算 n 个样本组成的 mini-batch 的梯度。可以看到：当 batch-size 为 1 是，其实就是SGD；而 batch-size 为 全部样本的数目，就是 batch gradient descent。  
$$
\theta = \theta - \eta · \nabla_{\theta}J(\theta;x^{(i:i+n)};y^{(i:i+n)})
$$
![mark](http://p0vxqvs23.bkt.clouddn.com/imgs/171213/3liCED2Iie.png?imageslim)

- 更新过程相比于 SGD 抖动比较小
- 每次迭代比 batch gradient descent 快

# 梯度下降优化算法

##	Learning rate decay

在迭代过程中，learning rate（用`η`来表示）的大小很重要：

- 迭代后期η太大：造成目标函数一直在最小值附近跳动，不能收敛
- 迭代前期η太小：收敛速度太慢

按照上面的叙述，η需要缓慢地减小，可以有这些规则来让η逐渐变小。

![mark](http://p0vxqvs23.bkt.clouddn.com/imgs/171213/49dm0D8ACf.png?imageslim)

